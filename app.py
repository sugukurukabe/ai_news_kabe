import streamlit as st
import google.generativeai as genai
import arxiv
import feedparser
import urllib.parse
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime
from time import mktime

# ==========================================
# 1. è¨­å®š & ãƒ‡ã‚¶ã‚¤ãƒ³ & èªè¨¼
# ==========================================
st.set_page_config(page_title="AI Intelligence Hub", page_icon="ğŸ§ ", layout="wide")

st.markdown("""
<style>
    .stApp { font-family: "Hiragino Kaku Gothic ProN", "Meiryo", sans-serif; }
    h1, h2, h3 { color: #2c3e50; }
    .source-tag { font-size: 0.8rem; color: #7f8c8d; }
    .saved-tag { background-color: #d4edda; color: #155724; padding: 2px 6px; border-radius: 4px; font-size: 0.8rem; }
    .date-badge { background-color: #f1f3f5; color: #495057; padding: 2px 6px; border-radius: 4px; font-size: 0.75rem; margin-left: 8px;}
    
    /* ã‚¹ãƒãƒ›ã§ã®ãƒœã‚¿ãƒ³æŠ¼ã—é–“é•ã„é˜²æ­¢ */
    div[data-testid="stButton"] button {
        width: 100%; 
    }
</style>
""", unsafe_allow_html=True)

# APIã‚­ãƒ¼ã¨ã‚·ãƒ¼ãƒˆè¨­å®š
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = dict(st.secrets["gcp_service_account"])
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    sheet = client.open("AI_Library_DB").sheet1
except Exception as e:
    st.error(f"âš ï¸ è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('models/gemini-flash-latest')

# ==========================================
# 2. å®šæ•°
# ==========================================
ARXIV_CATEGORIES = {
    "LLM / è‡ªç„¶è¨€èªå‡¦ç†": "cs.CL", "ç”»åƒç”Ÿæˆ / ãƒ“ã‚¸ãƒ§ãƒ³": "cs.CV",
    "ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ / ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": "cs.RO", "ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ / ã‚¨ãƒƒã‚¸AI": "cs.AR"
}
TECH_BLOGS = {
    "OpenAI": "https://openai.com/index.rss", "Anthropic": "https://www.anthropic.com/rss",
    "Google AI": "https://blog.google/technology/ai/rss/", "NVIDIA Blog": "https://blogs.nvidia.com/feed/",
    "Microsoft AI": "https://blogs.microsoft.com/ai/feed/"
}
NEWS_TOPICS = ["DeepMind", "Tesla AI", "SpaceX", "NVIDIA AI", "SoftBank AI"]

# ==========================================
# 3. é–¢æ•°ç¾¤
# ==========================================
def is_within_date_range(published_struct_time, days):
    if not published_struct_time: return True
    pub_date = datetime.fromtimestamp(mktime(published_struct_time))
    return (datetime.now() - pub_date).days <= days

def load_db():
    try: return sheet.get_all_records()
    except: return []

def save_to_db(item, memo):
    try:
        row = [item['id'], item['title'], item['url'], item['source'], datetime.now().strftime("%Y-%m-%d %H:%M"), memo]
        sheet.append_row(row)
        st.toast("ğŸ’¾ ä¿å­˜ã—ã¾ã—ãŸï¼", icon="â˜ï¸")
    except Exception as e:
        st.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def delete_from_db(item_id):
    try:
        cell = sheet.find(str(item_id))
        sheet.delete_rows(cell.row)
        st.toast("ğŸ—‘ï¸ å‰Šé™¤ã—ã¾ã—ãŸ", icon="ğŸ—‘ï¸")
    except Exception as e:
        st.error(f"å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

# ã€é‡è¦ã€‘ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã‚’è¡Œã†é–¢æ•°ã«å¤‰æ›´
def stream_analysis(text, source_type, placeholder):
    """AIè¦ç´„ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã—ã¦è¡¨ç¤º"""
    prompt = f"""
    ã‚ãªãŸã¯AIå°‚é–€ã®ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®{source_type}ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚
    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
    **é‡è¦åº¦:** [é«˜/ä¸­/ä½] | **åˆ†é‡:** [ã‚¿ã‚°]
    **è¦ç‚¹:**
    - [è¦ç‚¹1]
    - [è¦ç‚¹2]
    **ä¸€è¨€:** [æ ¸å¿ƒ]
    ãƒ†ã‚­ã‚¹ãƒˆ: {text[:8000]}
    """
    try:
        # stream=True ã§å°‘ã—ãšã¤å—ã‘å–ã‚‹
        response = model.generate_content(prompt, stream=True)
        full_text = ""
        for chunk in response:
            full_text += chunk.text
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’éšæ™‚æ›´æ–°ï¼ˆã“ã‚ŒãŒãƒ‘ãƒ©ãƒ‘ãƒ©è¡¨ç¤ºã®æ­£ä½“ï¼‰
            placeholder.markdown(full_text)
        return full_text
    except:
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def fetch_data(cats, blogs, news, days_range):
    items = []
    client = arxiv.Client()
    for c in cats:
        s = arxiv.Search(query=f"cat:{ARXIV_CATEGORIES[c]}", max_results=5, sort_by=arxiv.SortCriterion.SubmittedDate)
        for r in client.results(s):
            pub_date = r.published.replace(tzinfo=None)
            if (datetime.now() - pub_date).days <= days_range:
                items.append({"id": r.entry_id, "title": r.title, "source": "arXiv", "url": r.entry_id, "content": r.summary, "date": r.published.strftime("%Y-%m-%d"), "icon": "ğŸ“"})

    for b in blogs:
        try:
            f = feedparser.parse(TECH_BLOGS[b])
            for e in f.entries:
                if hasattr(e, 'published_parsed') and not is_within_date_range(e.published_parsed, days_range): continue
                items.append({"id": e.link, "title": e.title, "source": b, "url": e.link, "content": e.get("summary", "")[:1000], "date": "Blog", "icon": "ğŸ¢"})
                if len([x for x in items if x['source'] == b]) >= 3: break
        except: pass
            
    for n in news:
        try:
            term = f"{days_range}d"
            url = f"https://news.google.com/rss/search?q={urllib.parse.quote(n+' when:'+term)}&hl=en-US&gl=US&ceid=US:en"
            f = feedparser.parse(url)
            for e in f.entries[:3]:
                items.append({"id": e.link, "title": e.title, "source": "News", "url": e.link, "content": e.get("summary", ""), "date": "News", "icon": "ğŸŒ"})
        except: pass
    return items

# ==========================================
# 4. UIæ§‹ç¯‰
# ==========================================
with st.sidebar:
    st.title("ğŸ§  AI Intelligence Hub")
    
    st.header("ğŸ“… æœŸé–“æŒ‡å®š")
    date_options = {"24æ™‚é–“ä»¥å†…": 1, "3æ—¥ä»¥å†…": 3, "1é€±é–“ä»¥å†…": 7, "1ãƒ¶æœˆä»¥å†…": 30}
    selected_period = st.selectbox("æ¤œç´¢ç¯„å›²", list(date_options.keys()), index=2)
    days_range = date_options[selected_period]

    st.divider()
    page = st.radio("ãƒ¡ãƒ‹ãƒ¥ãƒ¼", ["ğŸ“¡ æ¢ç´¢", "â˜ï¸ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"])
    st.divider()

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ– ---
if 'generated_summaries' not in st.session_state:
    st.session_state.generated_summaries = {}

if page == "ğŸ“¡ æ¢ç´¢":
    st.header(f"æ¢ç´¢ãƒ•ã‚£ãƒ¼ãƒ‰ ({selected_period})")
    
    # DBèª­ã¿è¾¼ã¿ã¯é‡ã„ã®ã§æœ€åˆã ã‘ã«ã™ã‚‹å·¥å¤«ã‚‚å¯èƒ½ã ãŒã€ä»Šã¯ãã®ã¾ã¾
    db_data = load_db()
    saved_ids = [str(d['id']) for d in db_data]

    with st.expander("è©³ç´°æ¤œç´¢è¨­å®š", expanded=False):
        s_cats = st.multiselect("è«–æ–‡", list(ARXIV_CATEGORIES.keys()), ["LLM / è‡ªç„¶è¨€èªå‡¦ç†"])
        s_blogs = st.multiselect("ãƒ–ãƒ­ã‚°", list(TECH_BLOGS.keys()), ["OpenAI", "Anthropic"])
        s_news = st.multiselect("ãƒ‹ãƒ¥ãƒ¼ã‚¹", NEWS_TOPICS, ["NVIDIA AI"])
        
        if st.button("æƒ…å ±ã‚’æ›´æ–°ã™ã‚‹", type="primary", use_container_width=True):
            with st.spinner('è¨˜äº‹ã‚’é›†ã‚ã¦ã„ã¾ã™...'):
                st.session_state.feed_data = fetch_data(s_cats, s_blogs, s_news, days_range)
    
    if 'feed_data' in st.session_state:
        if not st.session_state.feed_data:
            st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        for item in st.session_state.feed_data:
            with st.container(border=True):
                st.markdown(f"**{item['icon']} {item['source']}** <span class='date-badge'>{selected_period}</span>", unsafe_allow_html=True)
                st.markdown(f"### {item['title']}")
                
                # ã“ã“ã‹ã‚‰å¤‰æ›´ï¼šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã®ãƒ­ã‚¸ãƒƒã‚¯
                # æ—¢ã«ç”Ÿæˆæ¸ˆã¿ãªã‚‰ãã‚Œã‚’è¡¨ç¤º
                if item['id'] in st.session_state.generated_summaries:
                    st.info(st.session_state.generated_summaries[item['id']])
                else:
                    # ã¾ã ç”Ÿæˆã—ã¦ã„ãªã„å ´åˆã€ç©ºã®ç®±ã‚’ç”¨æ„
                    result_placeholder = st.empty()
                    
                    if st.button("ğŸ¤– è§£èª¬ã‚’èª­ã‚€", key=f"btn_{item['id']}"):
                        # rerunã›ãšã«ã€ãã®å ´ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œï¼
                        full_text = stream_analysis(item['content'], item['source'], result_placeholder)
                        # ç”Ÿæˆçµ‚ã‚ã£ãŸã‚‰ä¿å­˜
                        st.session_state.generated_summaries[item['id']] = full_text
                        # å¿µã®ç‚ºã‚‚ã†ä¸€åº¦rerunã—ã¦ãƒœã‚¿ãƒ³ã®çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¦ã‚‚ã„ã„ãŒã€
                        # ã‚¹ãƒãƒ›ä½“é¨“å‘ä¸Šã®ãŸã‚rerunã›ãšã«ãã®ã¾ã¾ã«ã™ã‚‹
                
                # ä¿å­˜ãƒœã‚¿ãƒ³ã‚¨ãƒªã‚¢
                # è§£èª¬ãŒã‚ã‚‹å ´åˆã®ã¿ä¿å­˜ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
                if item['id'] in st.session_state.generated_summaries:
                    analysis = st.session_state.generated_summaries[item['id']]
                    c1, c2 = st.columns(2)
                    c1.link_button("ğŸ“„ åŸæ–‡ã¸", item['url'], use_container_width=True)
                    
                    if str(item['id']) in saved_ids:
                        c2.button("âœ… ä¿å­˜æ¸ˆã¿", disabled=True, use_container_width=True)
                    else:
                        if c2.button("ğŸ’¾ ã‚¯ãƒ©ã‚¦ãƒ‰ä¿å­˜", key=f"save_{item['id']}", type="primary", use_container_width=True):
                            save_to_db(item, analysis)
                            # ä¿å­˜å¾Œã¯ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ã€Œä¿å­˜æ¸ˆã¿ã€è¡¨ç¤ºã«å¤‰ãˆã‚‹
                            st.rerun()

    else:
        st.info("ä¸Šã®ã€Œè©³ç´°æ¤œç´¢è¨­å®šã€ã‹ã‚‰ã€Œæƒ…å ±ã‚’æ›´æ–°ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

elif page == "â˜ï¸ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª":
    st.header("ãƒã‚¤ãƒ©ã‚¤ãƒ–ãƒ©ãƒª")
    bookmarks = load_db()
    if not bookmarks:
        st.warning("ä¿å­˜ã•ã‚ŒãŸè¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        for item in bookmarks:
            with st.container(border=True):
                st.markdown(f"<span class='saved-tag'>{item['saved_at']}</span>", unsafe_allow_html=True)
                st.markdown(f"### {item['title']}")
                with st.expander("AIãƒ¡ãƒ¢"):
                    st.markdown(item['ai_memo'])
                c1, c2 = st.columns(2)
                c1.link_button("åŸæ–‡", item['url'], use_container_width=True)
                if c2.button("å‰Šé™¤", key=f"del_{item['id']}", use_container_width=True):
                    delete_from_db(item['id'])
                    st.rerun()
