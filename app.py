import streamlit as st
import google.generativeai as genai
import arxiv
import feedparser
import urllib.parse
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime
from time import mktime

# ==========================================
# 1. è¨­å®š & èªè¨¼
# ==========================================
st.set_page_config(page_title="AI Intelligence Hub", page_icon="ğŸ§ ", layout="wide")

st.markdown("""
<style>
    .stApp { font-family: "Hiragino Kaku Gothic ProN", "Meiryo", sans-serif; }
    h1, h2, h3 { color: #2c3e50; }
    .saved-tag { background-color: #d4edda; color: #155724; padding: 2px 6px; border-radius: 4px; font-size: 0.8rem; }
    .date-badge { background-color: #f1f3f5; color: #495057; padding: 2px 6px; border-radius: 4px; font-size: 0.75rem; margin-left: 8px;}
    div[data-testid="stButton"] button { width: 100%; }
</style>
""", unsafe_allow_html=True)

try:
    # Secretsã®èª­ã¿è¾¼ã¿ãƒã‚§ãƒƒã‚¯
    if "GOOGLE_API_KEY" not in st.secrets:
        st.error("Secretsã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEY ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()
    if "gcp_service_account" not in st.secrets:
        st.error("Secretsã‚¨ãƒ©ãƒ¼: [gcp_service_account] ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    API_KEY = st.secrets["GOOGLE_API_KEY"]
    
    # ã‚·ãƒ¼ãƒˆæ¥ç¶šè¨­å®š
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds_dict = dict(st.secrets["gcp_service_account"])
    
    # éµã®æ”¹è¡Œã‚³ãƒ¼ãƒ‰è£œæ­£
    if "private_key" in creds_dict:
        creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)

    # â–¼IDæŒ‡å®šã§ã‚ªãƒ¼ãƒ—ãƒ³
    SPREADSHEET_ID = "1w4Xa9XxdGH26OxUCbxX3rV8jhajEESccVlIfPy9Bbpk" 
    sheet = client.open_by_key(SPREADSHEET_ID).sheet1

except Exception as e:
    # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç”»é¢ã«å‡ºã™
    st.error(f"âš ï¸ èµ·å‹•ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
    # èªè¨¼æƒ…å ±ã®ã©ã®éƒ¨åˆ†ã§ã‚³ã‚±ãŸã‹ãƒ’ãƒ³ãƒˆã‚’å‡ºã™
    st.warning("ãƒ’ãƒ³ãƒˆ: Streamlit Cloudã®Secretsè¨­å®šã§ã€TOMLå½¢å¼ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('models/gemini-flash-latest')

# ==========================================
# 2. å®šæ•° & é–¢æ•°
# ==========================================
ARXIV_CATEGORIES = {
    "LLM / è‡ªç„¶è¨€èªå‡¦ç†": "cs.CL", "ç”»åƒç”Ÿæˆ / ãƒ“ã‚¸ãƒ§ãƒ³": "cs.CV",
    "ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ / ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": "cs.RO", "ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ / ã‚¨ãƒƒã‚¸AI": "cs.AR"
}
TECH_BLOGS = {
    "OpenAI": "https://openai.com/index.rss", "Anthropic": "https://www.anthropic.com/rss",
    "Google AI": "https://blog.google/technology/ai/rss/", "NVIDIA Blog": "https://blogs.nvidia.com/feed/"
}
NEWS_TOPICS = ["DeepMind", "Tesla AI", "SpaceX", "NVIDIA AI", "SoftBank AI"]

def is_within_date_range(published_struct_time, days):
    if not published_struct_time: return True
    pub_date = datetime.fromtimestamp(mktime(published_struct_time))
    return (datetime.now() - pub_date).days <= days

def load_db():
    try: return sheet.get_all_records()
    except: return []

def save_to_db(item, memo):
    try:
        row = [item['id'], item['title'], item['url'], item['source'], datetime.now().strftime("%Y-%m-%d %H:%M"), memo]
        sheet.append_row(row)
        st.toast("ğŸ’¾ ä¿å­˜ã—ã¾ã—ãŸï¼", icon="â˜ï¸")
    except Exception as e:
        st.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def delete_from_db(item_id):
    try:
        cell = sheet.find(str(item_id))
        sheet.delete_rows(cell.row)
        st.toast("ğŸ—‘ï¸ å‰Šé™¤ã—ã¾ã—ãŸ", icon="ğŸ—‘ï¸")
    except Exception as e:
        st.error(f"å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

def stream_analysis(text, source_type, placeholder):
    prompt = f"""
    ã‚ãªãŸã¯AIå°‚é–€ã®ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®{source_type}ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚
    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
    **é‡è¦åº¦:** [é«˜/ä¸­/ä½] | **åˆ†é‡:** [ã‚¿ã‚°]
    **è¦ç‚¹:**
    - [è¦ç‚¹1]
    - [è¦ç‚¹2]
    **ä¸€è¨€:** [æ ¸å¿ƒ]
    ãƒ†ã‚­ã‚¹ãƒˆ: {text[:8000]}
    """
    try:
        response = model.generate_content(prompt, stream=True)
        full_text = ""
        for chunk in response:
            full_text += chunk.text
            placeholder.markdown(full_text)
        return full_text
    except: return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def fetch_data(cats, blogs, news, days_range):
    items = []
    client = arxiv.Client()
    for c in cats:
        s = arxiv.Search(query=f"cat:{ARXIV_CATEGORIES[c]}", max_results=5, sort_by=arxiv.SortCriterion.SubmittedDate)
        for r in client.results(s):
            pub_date = r.published.replace(tzinfo=None)
            if (datetime.now() - pub_date).days <= days_range:
                items.append({"id": r.entry_id, "title": r.title, "source": "arXiv", "url": r.entry_id, "content": r.summary, "date": r.published.strftime("%Y-%m-%d"), "icon": "ğŸ“"})
    
    for b in blogs:
        try:
            f = feedparser.parse(TECH_BLOGS[b])
            for e in f.entries:
                if hasattr(e, 'published_parsed') and not is_within_date_range(e.published_parsed, days_range): continue
                items.append({"id": e.link, "title": e.title, "source": b, "url": e.link, "content": e.get("summary", "")[:1000], "date": "Blog", "icon": "ğŸ¢"})
                if len([x for x in items if x['source'] == b]) >= 3: break
        except: pass

    for n in news:
        try:
            term = f"{days_range}d"
            url = f"https://news.google.com/rss/search?q={urllib.parse.quote(n+' when:'+term)}&hl=en-US&gl=US&ceid=US:en"
            f = feedparser.parse(url)
            for e in f.entries[:3]:
                items.append({"id": e.link, "title": e.title, "source": "News", "url": e.link, "content": e.get("summary", ""), "date": "News", "icon": "ğŸŒ"})
        except: pass
    return items

# ==========================================
# 3. UIæ§‹ç¯‰
# ==========================================
with st.sidebar:
    st.title("ğŸ§  AI Intelligence Hub")
    st.header("ğŸ“… æœŸé–“æŒ‡å®š")
    date_options = {"24æ™‚é–“ä»¥å†…": 1, "3æ—¥ä»¥å†…": 3, "1é€±é–“ä»¥å†…": 7, "1ãƒ¶æœˆä»¥å†…": 30}
    selected_period = st.selectbox("æ¤œç´¢ç¯„å›²", list(date_options.keys()), index=2)
    days_range = date_options[selected_period]
    st.divider()
    page = st.radio("ãƒ¡ãƒ‹ãƒ¥ãƒ¼", ["ğŸ“¡ æ¢ç´¢", "â˜ï¸ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"])

if 'generated_summaries' not in st.session_state:
    st.session_state.generated_summaries = {}

if page == "ğŸ“¡ æ¢ç´¢":
    st.header(f"æ¢ç´¢ãƒ•ã‚£ãƒ¼ãƒ‰ ({selected_period})")
    
    try:
        db_data = load_db()
        saved_ids = [str(d['id']) for d in db_data]
    except:
        saved_ids = []

    with st.expander("è©³ç´°æ¤œç´¢è¨­å®š", expanded=False):
        s_cats = st.multiselect("è«–æ–‡", list(ARXIV_CATEGORIES.keys()), ["LLM / è‡ªç„¶è¨€èªå‡¦ç†"])
        s_blogs = st.multiselect("ãƒ–ãƒ­ã‚°", list(TECH_BLOGS.keys()), ["OpenAI", "Anthropic"])
        s_news = st.multiselect("ãƒ‹ãƒ¥ãƒ¼ã‚¹", NEWS_TOPICS, ["NVIDIA AI"])
        if st.button("æƒ…å ±ã‚’æ›´æ–°ã™ã‚‹", type="primary"):
            with st.spinner('è¨˜äº‹ã‚’é›†ã‚ã¦ã„ã¾ã™...'):
                st.session_state.feed_data = fetch_data(s_cats, s_blogs, s_news, days_range)
    
    if 'feed_data' in st.session_state:
        if not st.session_state.feed_data:
            st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        for item in st.session_state.feed_data:
            with st.container(border=True):
                st.markdown(f"**{item['icon']} {item['source']}** <span class='date-badge'>{selected_period}</span>", unsafe_allow_html=True)
                st.markdown(f"### {item['title']}")
                
                if item['id'] in st.session_state.generated_summaries:
                    st.info(st.session_state.generated_summaries[item['id']])
                else:
                    placeholder = st.empty()
                    if st.button("ğŸ¤– è§£èª¬ã‚’èª­ã‚€", key=f"btn_{item['id']}"):
                        full_text = stream_analysis(item['content'], item['source'], placeholder)
                        st.session_state.generated_summaries[item['id']] = full_text
                
                if item['id'] in st.session_state.generated_summaries:
                    analysis = st.session_state.generated_summaries[item['id']]
                    c1, c2 = st.columns(2)
                    c1.link_button("ğŸ“„ åŸæ–‡ã¸", item['url'], use_container_width=True)
                    if str(item['id']) in saved_ids:
                        c2.button("âœ… ä¿å­˜æ¸ˆã¿", disabled=True, use_container_width=True)
                    else:
                        if c2.button("ğŸ’¾ ã‚¯ãƒ©ã‚¦ãƒ‰ä¿å­˜", key=f"save_{item['id']}", type="primary", use_container_width=True):
                            save_to_db(item, analysis)
                            st.rerun()

elif page == "â˜ï¸ ãƒ©ã‚¤ãƒ–ãƒ©ãƒª":
    st.header("ãƒã‚¤ãƒ©ã‚¤ãƒ–ãƒ©ãƒª")
    bookmarks = load_db()
    if not bookmarks:
        st.warning("ä¿å­˜ã•ã‚ŒãŸè¨˜äº‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        for item in bookmarks:
            with st.container(border=True):
                st.markdown(f"<span class='saved-tag'>{item['saved_at']}</span>", unsafe_allow_html=True)
                st.markdown(f"### {item['title']}")
                with st.expander("AIãƒ¡ãƒ¢"):
                    st.markdown(item['ai_memo'])
                c1, c2 = st.columns(2)
                c1.link_button("åŸæ–‡", item['url'], use_container_width=True)
                if c2.button("å‰Šé™¤", key=f"del_{item['id']}", use_container_width=True):
                    delete_from_db(item['id'])
                    st.rerun()
