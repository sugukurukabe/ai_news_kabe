import streamlit as st
import google.generativeai as genai
import arxiv
import feedparser
import urllib.parse
from datetime import datetime
from time import mktime

# ==========================================
# 1. è¨­å®š
# ==========================================
st.set_page_config(page_title="AI Intelligence Hub", page_icon="ğŸ§ ", layout="wide")
st.markdown("""<style>.stApp{font-family:"Hiragino Kaku Gothic ProN",sans-serif;}h1,h2,h3{color:#2c3e50;}div[data-testid="stButton"] button{width:100%;}</style>""", unsafe_allow_html=True)

# ã‚¨ãƒ©ãƒ¼ã®åŸå› ã«ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å…¨å‰Šé™¤ã—ã¾ã—ãŸ
# APIã‚­ãƒ¼ã ã‘ã‚ã‚Œã°å‹•ãã¾ã™
try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    # ä¸‡ãŒä¸€SecretsãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼å›é¿
    st.error("è¨­å®šã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('models/gemini-flash-latest')

# ==========================================
# 2. å®šæ•° & é–¢æ•°
# ==========================================
ARXIV_CATEGORIES = {"LLM": "cs.CL", "Vision": "cs.CV", "Robotics": "cs.RO", "Hardware": "cs.AR"}
TECH_BLOGS = {"OpenAI": "https://openai.com/index.rss", "Anthropic": "https://www.anthropic.com/rss", "Google": "https://blog.google/technology/ai/rss/", "NVIDIA": "https://blogs.nvidia.com/feed/"}
NEWS_TOPICS = ["DeepMind", "Tesla AI", "SpaceX", "NVIDIA AI", "SoftBank AI"]

def is_within_date_range(published_struct_time, days):
    if not published_struct_time: return True
    pub_date = datetime.fromtimestamp(mktime(published_struct_time))
    return (datetime.now() - pub_date).days <= days

def stream_analysis(text, source_type, placeholder):
    try:
        response = model.generate_content(f"ã‚ãªãŸã¯AIå°‚é–€ç·¨é›†è€…ã§ã™ã€‚æ¬¡ã®{source_type}ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\nãƒ†ã‚­ã‚¹ãƒˆ: {text[:8000]}", stream=True)
        full_text = ""
        for chunk in response:
            full_text += chunk.text
            placeholder.markdown(full_text)
        return full_text
    except: return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def fetch_data(cats, blogs, news, days_range):
    items = []
    # arXiv
    client = arxiv.Client()
    for c in cats:
        try:
            s = arxiv.Search(query=f"cat:{ARXIV_CATEGORIES[c]}", max_results=5, sort_by=arxiv.SortCriterion.SubmittedDate)
            for r in client.results(s):
                pub_date = r.published.replace(tzinfo=None)
                if (datetime.now() - pub_date).days <= days_range:
                    items.append({"id": r.entry_id, "title": r.title, "source": "arXiv", "url": r.entry_id, "content": r.summary, "date": r.published.strftime("%Y-%m-%d"), "icon": "ğŸ“"})
        except: pass
    
    # Blogs
    for b in blogs:
        try:
            f = feedparser.parse(TECH_BLOGS[b])
            for e in f.entries:
                if hasattr(e, 'published_parsed') and not is_within_date_range(e.published_parsed, days_range): continue
                items.append({"id": e.link, "title": e.title, "source": b, "url": e.link, "content": e.get("summary", "")[:1000], "date": "Blog", "icon": "ğŸ¢"})
                if len([x for x in items if x['source'] == b]) >= 3: break
        except: pass
        
    # News
    for n in news:
        try:
            url = f"https://news.google.com/rss/search?q={urllib.parse.quote(n+' when:'+str(days_range)+'d')}&hl=en-US&gl=US&ceid=US:en"
            f = feedparser.parse(url)
            for e in f.entries[:3]:
                items.append({"id": e.link, "title": e.title, "source": "News", "url": e.link, "content": e.get("summary", ""), "date": "News", "icon": "ğŸŒ"})
        except: pass
    return items

# ==========================================
# 3. UIæ§‹ç¯‰
# ==========================================
with st.sidebar:
    st.title("ğŸ§  AI Intelligence Hub")
    days_range = st.selectbox("æœŸé–“", [1, 3, 7, 30], index=2, format_func=lambda x: f"{x}æ—¥ä»¥å†…")
    st.caption("â€»ç¾åœ¨ã€ä¿å­˜æ©Ÿèƒ½ã¯åœæ­¢ä¸­ã§ã™")

if 'gen_sums' not in st.session_state: st.session_state.gen_sums = {}

st.header(f"æ¢ç´¢ãƒ•ã‚£ãƒ¼ãƒ‰ ({days_range}æ—¥ä»¥å†…)")

if st.button("æƒ…å ±ã‚’æ›´æ–°ã™ã‚‹", type="primary"):
    with st.spinner("ä¸–ç•Œä¸­ã®AIæƒ…å ±ã‚’åé›†ä¸­..."):
        st.session_state.feed = fetch_data(ARXIV_CATEGORIES.keys(), TECH_BLOGS.keys(), NEWS_TOPICS, days_range)

if 'feed' in st.session_state:
    if not st.session_state.feed:
        st.info("æ–°ã—ã„è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœŸé–“ã‚’åºƒã’ã¦ã¿ã¦ãã ã•ã„ã€‚")
    
    for item in st.session_state.feed:
        with st.container(border=True):
            st.markdown(f"**{item['icon']} {item['source']}**")
            st.markdown(f"### {item['title']}")
            
            if item['id'] in st.session_state.gen_sums:
                st.info(st.session_state.gen_sums[item['id']])
            else:
                placeholder = st.empty()
                if st.button("ğŸ¤– AIè¦ç´„ã‚’èª­ã‚€", key=f"btn_{item['id']}"):
                    st.session_state.gen_sums[item['id']] = stream_analysis(item['content'], item['source'], placeholder)
            
            st.link_button("ğŸ“„ åŸæ–‡ã‚’èª­ã‚€", item['url'])

elif 'history' not in st.session_state:
    st.info("ã€Œæƒ…å ±ã‚’æ›´æ–°ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€æœ€æ–°ã®AIãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã—ã‚‡ã†ï¼")
