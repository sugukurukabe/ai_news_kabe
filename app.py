import streamlit as st
import google.generativeai as genai
import arxiv
import feedparser
import urllib.parse
from datetime import datetime
from time import mktime

# ==========================================
# 1. è¨­å®š & ã‚½ãƒ¼ã‚¹å®šç¾©
# ==========================================
st.set_page_config(page_title="Global AI News", page_icon="ğŸŒ", layout="wide")
st.markdown("""<style>.stApp{font-family:"Hiragino Kaku Gothic ProN",sans-serif;}h1,h2,h3{color:#2c3e50;}div[data-testid="stButton"] button{width:100%;}</style>""", unsafe_allow_html=True)

try:
    API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("è¨­å®šã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('models/gemini-flash-latest')

# --- â–¼â–¼â–¼ ã“ã“ã§æ–°ã—ã„ã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ ã—ã¾ã—ãŸ â–¼â–¼â–¼ ---
ARXIV_CATEGORIES = {
    "LLM / è¨€èªãƒ¢ãƒ‡ãƒ«": "cs.CL", 
    "Vision / ç”»åƒç”Ÿæˆ": "cs.CV", 
    "Robotics / ãƒ­ãƒœãƒƒãƒˆ": "cs.RO", 
    "AI General / å…¨èˆ¬": "cs.AI"
}

# ä¸»è¦AIä¼æ¥­ã®å…¬å¼ãƒ–ãƒ­ã‚°RSS
TECH_BLOGS = {
    "OpenAI": "https://openai.com/index.rss",
    "Anthropic (Claude)": "https://www.anthropic.com/rss",
    "Google DeepMind": "https://deepmind.google/blog/rss.xml", # DeepMindå°‚ç”¨
    "Google AI": "https://blog.google/technology/ai/rss/",
    "NVIDIA": "https://blogs.nvidia.com/feed/",
    "Microsoft Azure AI": "https://azure.microsoft.com/en-us/blog/feed/topics/artificial-intelligence/",
    "AWS Machine Learning": "https://aws.amazon.com/blogs/machine-learning/feed/"
}

# DeepSeekãªã©ã¯RSSãŒãªã„å ´åˆãŒå¤šã„ã®ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«è¿½åŠ 
NEWS_TOPICS = [
    "DeepSeek",       # ä¸­å›½ã®æ³¨ç›®AI
    "Qwen Alibaba",   # ã‚¢ãƒªãƒãƒã®AI
    "OpenAI o1",      # æœ€æ–°ãƒ¢ãƒ‡ãƒ«
    "Gemini 1.5",     # Google
    "Claude 3.5",     # Anthropic
    "Meta Llama 3",   # Meta
    "Sakana AI"       # æ—¥æœ¬ç™ºAI
]

# ==========================================
# 2. é–¢æ•°ç¾¤
# ==========================================
def is_within_date_range(published_struct_time, days):
    if not published_struct_time: return True
    pub_date = datetime.fromtimestamp(mktime(published_struct_time))
    return (datetime.now() - pub_date).days <= days

def stream_analysis(text, source_type, placeholder):
    try:
        # æ—¥æœ¬èªã§è¦ç´„ã™ã‚‹ã‚ˆã†ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´
        prompt = f"""
        ã‚ãªãŸã¯ãƒ—ãƒ­ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®{source_type}ã®å†…å®¹ã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
        å°‚é–€ç”¨èªã¯ãªã‚‹ã¹ãæ®‹ã—ã¤ã¤ã€åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãè§£èª¬ã—ã¦ãã ã•ã„ã€‚
        
        ## ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        **3è¡Œã¾ã¨ã‚:**
        - [è¦ç‚¹1]
        - [è¦ç‚¹2]
        - [è¦ç‚¹3]
        
        **è©³ç´°:** [å†…å®¹ã®è¦ç´„]
        
        å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ: {text[:10000]}
        """
        response = model.generate_content(prompt, stream=True)
        full_text = ""
        for chunk in response:
            full_text += chunk.text
            placeholder.markdown(full_text)
        return full_text
    except: return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def fetch_data(cats, blogs, news, days_range):
    items = []
    
    # 1. arXiv (è«–æ–‡)
    client = arxiv.Client()
    for c in cats:
        try:
            s = arxiv.Search(query=f"cat:{ARXIV_CATEGORIES[c]}", max_results=3, sort_by=arxiv.SortCriterion.SubmittedDate)
            for r in client.results(s):
                pub_date = r.published.replace(tzinfo=None)
                if (datetime.now() - pub_date).days <= days_range:
                    items.append({
                        "id": r.entry_id, # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ID
                        "title": r.title,
                        "source": "arXiv",
                        "url": r.entry_id,
                        "content": r.summary,
                        "date": r.published.strftime("%Y-%m-%d"),
                        "icon": "ğŸ“"
                    })
        except: pass
    
    # 2. Tech Blogs (ä¼æ¥­ãƒ–ãƒ­ã‚°)
    for name, url in blogs.items():
        try:
            f = feedparser.parse(url)
            for e in f.entries:
                if hasattr(e, 'published_parsed') and not is_within_date_range(e.published_parsed, days_range): continue
                items.append({
                    "id": e.link,
                    "title": e.title,
                    "source": name,
                    "url": e.link,
                    "content": e.get("summary", "")[:1500] + "...",
                    "date": "Blog",
                    "icon": "ğŸ¢"
                })
                if len([x for x in items if x['source'] == name]) >= 3: break
        except: pass
        
    # 3. Google News Search (DeepSeekãªã©ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹)
    for n in news:
        try:
            # è‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ–¹ãŒæƒ…å ±ãŒæ—©ã„ãŸã‚ en-US ã§æ¤œç´¢
            term = f"{days_range}d"
            rss_url = f"https://news.google.com/rss/search?q={urllib.parse.quote(n+' when:'+term)}&hl=en-US&gl=US&ceid=US:en"
            f = feedparser.parse(rss_url)
            for e in f.entries[:2]: # å„ãƒˆãƒ”ãƒƒã‚¯2ä»¶ã¾ã§
                items.append({
                    "id": e.link,
                    "title": e.title,
                    "source": f"News ({n})",
                    "url": e.link,
                    "content": e.get("summary", ""),
                    "date": "News",
                    "icon": "ğŸŒ"
                })
        except: pass
    
    return items

# ==========================================
# 3. UIæ§‹ç¯‰
# ==========================================
with st.sidebar:
    st.title("ğŸŒ Global AI News")
    days_range = st.selectbox("æœŸé–“", [1, 3, 7, 30], index=1, format_func=lambda x: f"{x}æ—¥ä»¥å†…")
    st.info("DeepSeek, DeepMind, OpenAI, Anthropicç­‰ã®æœ€æ–°æƒ…å ±ã‚’åé›†ã—ã¾ã™ã€‚")

if 'gen_sums' not in st.session_state: st.session_state.gen_sums = {}

st.header(f"æ¢ç´¢ãƒ•ã‚£ãƒ¼ãƒ‰ ({days_range}æ—¥ä»¥å†…)")

if st.button("æƒ…å ±ã‚’æ›´æ–°ã™ã‚‹", type="primary"):
    with st.spinner("ä¸–ç•Œä¸­ã®AIè«–æ–‡ãƒ»ãƒ–ãƒ­ã‚°ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­..."):
        raw_data = fetch_data(ARXIV_CATEGORIES.keys(), TECH_BLOGS, NEWS_TOPICS, days_range)
        
        # â–¼â–¼â–¼ã€é‡è¦ä¿®æ­£ã€‘é‡è¤‡å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯ â–¼â–¼â–¼
        # IDãŒåŒã˜è¨˜äº‹ã¯1ã¤ã«ã¾ã¨ã‚ã‚‹
        seen_ids = set()
        unique_data = []
        for item in raw_data:
            if item['id'] not in seen_ids:
                unique_data.append(item)
                seen_ids.add(item['id'])
        
        st.session_state.feed = unique_data

if 'feed' in st.session_state:
    if not st.session_state.feed:
        st.warning("è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœŸé–“ã‚’åºƒã’ã‚‹ã‹ã€æ›´æ–°ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    
    # enumerate(i) ã‚’ä½¿ã£ã¦ã€é€šã—ç•ªå·ã‚’å–å¾—
    for i, item in enumerate(st.session_state.feed):
        with st.container(border=True):
            st.markdown(f"**{item['icon']} {item['source']}**")
            st.markdown(f"### {item['title']}")
            
            # è¦ç´„ã‚¨ãƒªã‚¢
            if item['id'] in st.session_state.gen_sums:
                st.success(st.session_state.gen_sums[item['id']])
            else:
                placeholder = st.empty()
                # â–¼â–¼â–¼ã€ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã€‘keyã«ç•ªå·(i)ã‚’å«ã‚ã¦çµ¶å¯¾ã«é‡è¤‡ã•ã›ãªã„ â–¼â–¼â–¼
                if st.button("ğŸ¤– AIè¦ç´„ã‚’èª­ã‚€", key=f"btn_{i}_{item['id']}"):
                    st.session_state.gen_sums[item['id']] = stream_analysis(item['content'], item['source'], placeholder)
            
            st.link_button("ğŸ“„ åŸæ–‡ã‚’èª­ã‚€", item['url'])

elif 'history' not in st.session_state:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§æœŸé–“ã‚’é¸ã‚“ã§ã€Œæƒ…å ±ã‚’æ›´æ–°ã™ã‚‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
